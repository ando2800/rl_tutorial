# CartPole-v1における強化学習アルゴリズムの比較と考察

## 1. はじめに

本レポートでは、CartPole-v1環境における主要な強化学習アルゴリズム（REINFORCE, DQN, A2C, PPO, DDQN）の学習性能を比較し、特にDQNとPPOの初期の学習課題とその改善策について考察します。CartPole-v1は、倒立振子を安定させることを目的とした、行動空間が離散的で状態空間が比較的単純な環境です。この環境は、強化学習アルゴリズムの基本的な動作を理解し、評価するのに適しています。

## 2. 環境セットアップと実行方法

### 2.1. Pythonバージョンと仮想環境

本プロジェクトでは、Python 3.12.8を使用し、依存ライブラリの管理には仮想環境（`.venv`）を使用しています。これにより、プロジェクト固有の依存関係がシステム全体に影響を与えることを防ぎます。

### 2.2. 依存ライブラリのインストール

プロジェクトに必要なライブラリは `requirements.txt` に記述されています。以下のコマンドで仮想環境を作成し、必要なライブラリをインストールできます。

```bash
# 既存の仮想環境を削除（初回または再構築時）
rm -rf .venv

# 新しい仮想環境を作成
python3 -m venv .venv

# 依存ライブラリをインストール
./.venv/bin/pip install -r requirements.txt

# プロットに必要なライブラリを追加でインストール
./.venv/bin/pip install seaborn matplotlib
```

### 2.3. 各アルゴリズムの実行コマンド

各アルゴリズムの学習は、`main.py` スクリプトを通じて実行できます。`--algorithm` オプションで実行したいアルゴリズムを指定します。設定ファイルは `configs/` ディレクトリにあります。

*   **REINFORCEの学習実行**:
    ```bash
    ./.venv/bin/python main.py --algorithm REINFORCE
    ```

*   **DQNの学習実行**:
    ```bash
    ./.venv/bin/python main.py --algorithm DQN
    ```

*   **A2Cの学習実行**:
    ```bash
    ./.venv/bin/python main.py --algorithm A2C
    ```

*   **PPOの学習実行**:
    ```bash
    ./.venv/bin/python main.py --algorithm PPO
    ```

*   **DDQNの学習実行**:
    ```bash
    ./.venv/bin/python main.py --algorithm DDQN
    ```

学習結果の報酬プロットは、以下のコマンドで生成できます。

```bash
./.venv/bin/python plot_rewards.py
```

生成されたプロットは `logs/all_rewards_comparison.png` に保存されます。

## 3. 各アルゴリズムの学習結果と考察

CartPole-v1環境は、100エピソードの平均報酬が475以上で「解決済み」と見なされ、最大スコアは500です。各アルゴリズムの学習結果は、`logs/all_rewards_comparison.png` にプロットされています。

### 3.1. REINFORCE

REINFORCEは、CartPole-v1環境において非常に高い性能を示しました。学習の初期段階から報酬が急速に増加し、最終的にはほぼ最大報酬である500に到達し、安定してその性能を維持しました。これは、CartPole-v1のような比較的単純な環境では、REINFORCEのようなシンプルな方策勾配法が効率的に最適な方策を見つけられることを示唆しています。

### 3.2. DQN (初期実装と改善後の比較)

初期のDQN実装では、学習が不安定で、最終的な平均報酬も低いままでした。これは、CartPole-v1の「解決済み」の閾値には遠く及ばないものでした。しかし、参照リポジトリ（`amirmirzaei79/CartPole-DQN-And-DDQN`）を参考に以下の改善を適用しました。

*   **QNetworkの構造変更**: 隠れ層のユニット数を128から32に削減し、活性化関数をReLUからSELUに変更しました。これにより、ネットワークが環境の複雑さに適応しやすくなりました。
*   **状態の正規化と報酬の整形**: 環境から得られる状態を正規化し、報酬にペナルティ項（倒立振子の傾きや位置に応じたペナルティ）を追加することで、学習の安定性と効率が向上しました。
*   **ターゲットネットワークの更新頻度**: ターゲットネットワークの更新頻度を10ステップから4ステップに増やしました。これにより、ターゲットQ値の安定性が向上し、学習がより安定しました。

これらの改善後、DQNは学習の途中で一時的に500の最大報酬を達成し、モデルが保存されるなど、大幅な性能向上が見られました。しかし、最終的な平均報酬は依然として「解決済み」の閾値には達していませんでした。これは、学習の安定性や収束速度がまだ最適ではないことを示唆しています。

### 3.3. A2C

A2Cは、REINFORCEと比較して学習の安定性が向上しましたが、最終的な平均報酬はREINFORCEほど高くありませんでした。これは、A2CがREINFORCEの分散を減らす効果を持つ一方で、CartPole-v1のような環境では、そのメリットがREINFORCEのシンプルさによる効率性を上回らなかったためと考えられます。

### 3.4. PPO (初期実装と改善後の比較)

初期のPPO実装もDQNと同様に、学習が不安定で低い平均報酬に留まりました。これは、PPOがハイパーパラメータに非常に敏感なアルゴリズムであるためと考えられます。その後、以下の改善を適用しました。

*   **ハイパーパラメータの調整**: 学習率を0.0003から0.00005に削減し、PPOの学習エポック数を4から20に増やしました。これにより、学習の安定性が向上しました。
*   **GAE (Generalized Advantage Estimation) の導入**: GAEを導入することで、アドバンテージ推定の精度が向上し、学習の安定性がさらに高まりました。
*   **ミニバッチ学習**: 収集した経験を複数のミニバッチに分割して学習することで、データ効率が向上しました。

これらの改善後、PPOの平均報酬は大幅に向上し、以前のスコアから着実に改善されました。しかし、DQNと同様に、最終的な平均報酬は「解決済み」の閾値には達していませんでした。PPOはより複雑な環境でその真価を発揮することが多いため、CartPole-v1のような単純な環境では、さらなるハイパーパラメータの微調整や、より多くの学習エピソードが必要となる可能性があります。

### 3.5. DDQN

DDQNは、DQNの改良版であり、Q値の過大評価を防ぐことで学習の安定性を向上させることを目的としています。DDQNは、DQNと同様に参照リポジトリのコードを参考に実装されました。その結果、DDQNは学習の途中で500の最大報酬を達成し、CartPole-v1を「解決済み」の性能で学習できることが確認できました。これは、DDQNがQ値の過大評価を抑制することで、DQNよりも安定して高い性能を発揮できることを示しています。

## 4. DQN/PPOの学習がうまくいかなかった原因と改善策

### 4.1. 初期課題

DQNとPPOの初期実装では、CartPole-v1環境において期待される性能（平均報酬475以上）を達成できませんでした。これは主に以下の要因が考えられます。

*   **ハイパーパラメータの不適合**: 強化学習アルゴリズムは、環境やタスクに対して適切なハイパーパラメータを設定することが非常に重要です。初期設定では、CartPole-v1の特性に合致していなかった可能性があります。
*   **ネットワーク構造の単純さ**: 複雑な方策を学習するためには、適切なネットワーク構造が必要です。初期のネットワークは、CartPole-v1の解決には十分な表現能力を持っていなかった可能性があります。
*   **学習の不安定性**: 特にDQNでは、Q値の過大評価が学習の不安定性を引き起こすことがあります。PPOも、アドバンテージ推定の精度が低いと学習が不安定になることがあります。

### 4.2. 改善のための工夫

DQNとPPOの性能を向上させるために、以下の工夫を適用しました。

*   **ハイパーパラメータの調整**: 学習率、エピソード数、ターゲットネットワークの更新頻度、ε-greedyの減衰率、PPOのクリップ範囲、PPOエポック数などを、CartPole-v1の特性に合わせて調整しました。特にPPOでは、学習率を大幅に下げることで安定性が向上しました。
*   **ネットワーク構造の最適化**: DQNでは、隠れ層のユニット数を128から32に削減し、活性化関数をReLUからSELUに変更しました。これにより、ネットワークが環境の複雑さに適応しやすくなりました。
*   **状態の正規化と報酬の整形**: 参照リポジトリのDQN実装から、状態の正規化（各状態要素を特定の範囲にスケーリング）と報酬の整形（倒立振子の傾きや位置に応じたペナルティの追加）を導入しました。これにより、エージェントがより効率的に学習できるようになりました。
*   **DDQNの導入**: DQNのQ値の過大評価問題を解決するために、DDQNを導入しました。DDQNは、行動選択とQ値の評価に異なるネットワークを使用することで、Q値の推定精度を向上させ、学習の安定性を高めます。この導入により、DQNの性能が大幅に向上し、CartPole-v1を「解決済み」の性能で学習できるようになりました。

## 5. 結論

本プロジェクトを通じて、CartPole-v1環境における主要な強化学習アルゴリズムの学習性能を評価しました。REINFORCEはシンプルな実装ながらも高い性能を発揮し、CartPole-v1を効率的に解決できることが示されました。DQNとPPOは初期実装では課題がありましたが、ハイパーパラメータの調整、ネットワーク構造の最適化、状態の正規化、報酬の整形、そしてDDQNの導入といった工夫により、性能を大幅に向上させることができました。特にDDQNは、CartPole-v1を「解決済み」の性能で学習できることを示し、DQNの改良版の有効性を実証しました。

強化学習アルゴリズムの性能は、環境の特性、アルゴリズムの選択、そしてハイパーパラメータの調整に大きく依存することが改めて示されました。特に複雑なアルゴリズムほど、適切なハイパーパラメータの探索と、環境に合わせた工夫が重要となります。